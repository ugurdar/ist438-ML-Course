---
title: "Machine Learning Methods andApplications HW - Week 2"
author: "Uğur DAR"
date: "08 03 2021"
output:
  pdf_document:
    toc: true
    fig_caption: yes
---

# Linear Regression Model - Boston Housing Data



## Abstract 

This week's homework is linear regression, one of the simplest models in machine learning and statistical learning. In my last week's paper, I showed that there is a linear relationship between the medv target variable and some of the other variables in the Boston data set. In this document, fitting the linear regression model to the Boston data set, interpretation of the model outputs can be found.




## Packages


```{r message=FALSE, warning=FALSE}
library(dplyr)
library(mlbench)
library(car)
library(caret) 
library(lmtest)
```


\newpage

```{r}
data(BostonHousing) # Calling the data from mlbench
```

## Train-Test Split

```{r}
glimpse(BostonHousing)
```

Different variables can be selected as target variables in this data set. Crime rates in different neighborhoods in the city of Boston can be modeled. For this, the variable crime can be selected as the target variable. Determining the target variable is the subject of the research. In this data set, the properties of the houses and neighborhoods are given and the main purpose is to estimate the prices of the houses. So, I chose **medv**(	median value of owner-occupied homes in USD 1000's) variable as target variable. 

## Train-Test Split

```{r message=FALSE, warning=FALSE}
set.seed(26) # reproducbility 
index <- sample(nrow(BostonHousing),nrow(BostonHousing)*0.8)
train <- BostonHousing[index,]
test <- BostonHousing[-index,]
```


```{r}
dim(train)
```
```{r}
dim(test)
```

BostonHousing data has 506 instances(rows). 0.8 of the data is train, 0.2 of the data is test set. So, I choose randomly 404 instances from the data set as train data, 102 instances as test data.


\newpage

## Modelling

```{r}
model1 <- lm(medv~., data = train)
summary(model1)
```
$\widehat{m e d v}=4 1.903426-0.102668  * crim+ 0.044163 * zn+0.008889 * indus + 2.191928 * chas1-17.159257 *nox+3.299570  * rm+0.008360*age -1.391811 *dis+0.347897 *rad -0.014009* tax -1.070237  * ptratio + 0.008403 * b -0.563333  *lstat$ 

R gives a very good regression model output. Firstly, it gives some statistics about residuals. Secondly, we can see coefficients part. In this output, we see the coefficients estimated in the regression model, the standard deviation of these coefficients, the t statistisc and the test result of the coefficient significance, the p-value. We see stars sign for each coefficient next to p-value. This points to the Signif.code section below the output, for example, zn is significant feature at 0.05 significance level or crim is significant at 0.01 significance level. So, all features except indus and age significant at 0.05 level. Lastly, this section is about the significance of the model in general. As we can see, $R^{2}=0.734$, $R_{A d j}^{2} = 0.7252$. Theoretically, no matter how many explanatory variables we add to the model, the value of  $R^{2}$ in the model increases or remains constant. Therefore $R_{A d j}^{2}$ gives us more reliable results. In summary, in this model, the target variable is explained by the features at a rate of 0.7348. In general, the F test is used for the significance of the model. The last part shows the F statistics and the p-value in the F-test, $2.210^{-16}<0.001$ it is too close to 0, therefore we can say that the model is significance at 0.001 level.

\newpage

## Regression Diagnostics

Potential Problems in RMs

1. Non-linearity of the target-feature relationships
2. Correlation of error terms
3. Non-constant variance of error terms
4. Outliers
5. High-leverage points



Last week, I examined relationships between target and features. In that homework, plots shows that some features and target have linear relationship. Also, linear regression model is not seems to bad. It is significant and it explains 72.52% of the relationship. 
[See the HW1](https://github.com/ugurdar/ist438-ML-Course/blob/main/Homeworks/week1/u%C4%9Fur.pdf)



```{r}
par(mfrow=c(2,2))
plot(model1)
```
Residuals vs Fitted plot shows that, features and target have linear relationships but it hasn't exactly straight red line. So, non-linear models can also be tried. As we can see at Q-Q plot, the dots are supposed to follow a more or less straight line, which they clearly don’t here, residuals are not exactly normally distributed. Scale-Location plot, we check for homoskedasticity  we would want the red line on the plot to be more or less straight and horizontal, homoskedasticity(constant variance) assumptions can be considered to be fulfilled. Residuals vs Levarage plot shows that there is no leverage point, every instances past dotted red lines. In a nutshell, looking at the plot, we can't say that there is a problem with our model, but we need to do the necessary tests for assumptions.

### Multicollinearity

Multicollinearity can examine with VIF(Variance Inflation Factors). If VIF is 1, then there is no correlation, if it’s between 1 and 10, there is moderate correlation, and if it’s greater than 10, there is high correlation and there is serious multicollinearity problem.

```{r}
vif(model1)  
```

*tax* and *rad* features can cause  multicollinearity problem. Let's look at on plot.

```{r}
plot(BostonHousing$rad,BostonHousing$tax)
```

There does not appear to be a linear relationship between these variables in the plot. Let's look correlation matrix. 

```{r}
cor(BostonHousing %>% select_if(is.numeric)) > 0.90  # if correlation between features greater than 0.9 it turns TRUE
```

```{r}
cor(BostonHousing$rad,BostonHousing$tax)
```

As I said above, visual comments are subjective. There seems to be a very high correlation between *rad* and *tax*. One of these features can be omitted from the model.

\newpage


### Homoskedasticity \newline


```{r}
plot(model1$residuals)
```


```{r}
bptest(model1)
```

If the test statistic has a p-value below an appropriate threshold (e.g. p < 0.05) then the null hypothesis of homoskedasticity is rejected and heteroskedasticity assumed. 

```{r fig.height=5, fig.width=4}
model2 <- lm(log(medv)~.,data=train)
plot(model2$residuals)
```

```{r}
bptest(model2)
```


It doesn't work. Log transformation might use on other features. Other models can also be tested, even if one of the assumptions is violated, the linear regression model may not give bad results compared to other models.


## Model Evaluation 

### Some Evaluation Metrics \newline

$e_{t}=y_{t}-\hat{y}_{t}$ \newline
Mean squared error(MAE) $\quad \mathrm{MSE}=\frac{1}{n} \sum_{t=1}^{n} e_{t}^{2}$ \newline
Mean absolute error(MSE) $\operatorname{MAE}=\frac{1}{n} \sum_{t=1}^{n}\left|e_{t}\right|$ \newline
Root mean squared error(RMSE) $\quad \mathrm{RMSE}=\sqrt{\frac{1}{n} \sum_{t=1}^{n} e_{t}^{2}}$ \newline



```{r}
mse <- function(y_actual,y_pred){
  mean((y_actual-y_pred)^2)
}
rmse <- function(y_actual,y_pred){
  sqrt(mean((y_actual-y_pred)^2))
}
mae <- function(y_actual,y_pred){
  mean(abs(y_actual-y_pred))
}
```

### Prediction 

```{r}
train_pred <- predict(model1,train)
test_pred <- predict(model1,test)
```


### Evaluation

```{r}
train_metrics <- data.frame(MSE = mse(train$medv,train_pred),
                            RMSE = rmse(train$medv,train_pred),
                            MAE = mae(train$medv,train_pred))

test_metrics <- data.frame(MSE = mse(test$medv,test_pred),
                            RMSE = rmse(test$medv,test_pred),
                            MAE = mae(test$medv,test_pred))

results <- data.frame(rbind(train_metrics,test_metrics))
rownames(results) <- c("Train","Test")
results
```

The results came out close to each other. Maybe underfitting has occurred, because train error metrics greater than test's metrics.

The linear regression model, some assumptions have been violated like normality of error terms, multicollineratiy and homoskedasticity but when I try other regression models, it can be seen that linear regression works well.

[Click to see the other models on my Kaggle Notebook](https://www.kaggle.com/ugurdar/boston-housing-regression-with-caret-in-r)



